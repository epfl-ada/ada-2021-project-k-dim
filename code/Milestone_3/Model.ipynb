{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"Model.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# Needed imports\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from typing import Tuple, List\n","from functools import partial\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, RandomSampler\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.cuda.amp import GradScaler, autocast\n","\n","! pip install transformers\n","from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertPreTrainedModel\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score, confusion_matrix\n","from tqdm import tqdm\n","import itertools\n","import time\n","import seaborn as sns\n","import sys\n","import os"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"CyLUXMZRYEG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define dataset class\n","class QuoteDataset(Dataset):\n","    def __init__(self, tokenizer: BertTokenizer, dataframe: pd.DataFrame, class_columns: list, q_length: int = 300, lazy: bool = False):\n","        self.tokenizer = tokenizer\n","        self.pad_idx = tokenizer.pad_token_id\n","        self.class_columns = class_columns\n","        self.q_length = q_length\n","        self.lazy = lazy\n","        if not self.lazy:\n","            self.X = []\n","            self.Y = []\n","            for i, (row) in tqdm(dataframe.iterrows()):\n","                x, y = self.row_to_tensor(self.tokenizer, row, self.class_columns, self.q_length)\n","                self.X.append(x)\n","                self.Y.append(y)\n","        else:\n","            self.df = dataframe        \n","    \n","    @staticmethod\n","    def row_to_tensor(tokenizer: BertTokenizer, row: pd.Series, class_columns: list, q_length: int = 300) -> Tuple[torch.LongTensor, torch.LongTensor]:\n","        tokens = tokenizer.encode(row[\"quotation\"], add_special_tokens=True)\n","        if len(tokens) > q_length:\n","            tokens = tokens[:q_length-1] + [tokens[-1]]\n","        x = torch.LongTensor(tokens)\n","        y = torch.FloatTensor(row[class_columns])\n","        return x, y\n","        \n","    \n","    def __len__(self):\n","        if self.lazy:\n","            return len(self.df)\n","        else:\n","            return len(self.X)\n","\n","    def __getitem__(self, index: int) -> Tuple[torch.LongTensor, torch.LongTensor]:\n","        if not self.lazy:\n","            return self.X[index], self.Y[index]\n","        else:\n","            return self.row_to_tensor(self.tokenizer, self.df.iloc[index], self.class_columns, self.q_length)\n","            \n","\n","def collate_fn(batch: List[Tuple[torch.LongTensor, torch.LongTensor]], device: torch.device) \\\n","        -> Tuple[torch.LongTensor, torch.LongTensor]:\n","    x, y = list(zip(*batch))\n","    x = pad_sequence(x, batch_first=True, padding_value=0)\n","    y = torch.stack(y)\n","    return x.to(device), y.to(device)"],"metadata":{"id":"VR4yaTGpZ5yL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define model class\n","class BertClassifier(nn.Module):\n","    def __init__(self, bert: BertModel, num_classes: int):\n","        super().__init__()\n","        self.bert = bert\n","        self.classifier = nn.Linear(bert.config.hidden_size, num_classes)\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None, criterion=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask,\n","                            token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask)\n","        cls_output = outputs[1]\n","        cls_output = self.classifier(cls_output)\n","        criterion = criterion if criterion != None else nn.BCEWithLogitsLoss()\n","        loss = -1.\n","        if labels is not None:\n","            loss = criterion(cls_output, labels)\n","        return loss, cls_output\n","\n","def train(model, iterator, optimizer, scheduler, criterion=None):\n","    model.train()\n","    total_loss = 0\n","    scaler = GradScaler()\n","    for x, y in tqdm(iterator):\n","        with autocast():\n","            mask = (x != 0).float()\n","            loss, outputs = model(x, attention_mask=mask, labels=y, criterion=criterion)\n","            loss = loss / iters_to_accumulate\n","            with torch.no_grad():\n","                total_loss += loss.item()\n","        if next(model.parameters()).is_cuda:\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","        else:\n","            loss.backward()\n","            optimizer.step()\n","        optimizer.zero_grad()\n","        scheduler.step()\n","    print(f\"Train loss {total_loss / len(iterator)}\")\n","\n","def evaluate(model, iterator, class_columns, criterion=None):\n","    model.eval()\n","    pred = []\n","    true = []\n","    with torch.no_grad():\n","        total_loss = 0\n","        for x, y in tqdm(iterator):\n","            mask = (x != 0).float()\n","            loss, outputs = model(x, attention_mask=mask, labels=y, criterion=criterion)\n","            outputs = torch.sigmoid(outputs)\n","            total_loss += loss\n","            true += y.cpu().numpy().tolist()\n","            pred += outputs.cpu().numpy().tolist()\n","    true = np.array(true)\n","    pred = np.array(pred)\n","    # print ...\n","    print(f\"Amount of eval. samples: {np.sum(true)}\")\n","    for i, name in enumerate(class_columns):\n","        amount_i = np.sum(true[:, i])\n","        print(f\"Amount of {name}: {amount_i}\")\n","        if amount_i > 1 and amount_i < true.shape[0] - 1:\n","            print(f\"{name} roc_auc {roc_auc_score(true[:, i], pred[:, i])}\")\n","        else:\n","            print(f\"{name} roc_auc ---\")\n","    print(f\"Evaluate loss {total_loss / len(iterator)}\")\n","    return total_loss.cpu().numpy() / len(iterator), true, pred\n","\n","def save_model(model, optimizer, scheduler, eval_loss, epoch, chunk_number, delta_time):\n","    # save model and optimizer params\n","    print(f\"Start saving the model: epoch {epoch}, chunk_number {chunk_number}, time {delta_time}\")\n","    PATH = \"model_state_dict\"\n","    torch.save({\n","                'time': delta_time,\n","                'epoch': epoch,\n","                'chunk_number': chunk_number,\n","                'eval_loss': eval_loss,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'scheduler_state_dict': scheduler.state_dict()\n","                }, PATH)\n","    print(\"The model is saved\")"],"metadata":{"id":"k2LQCkhndcr1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define useful functions\n","def set_seed(seed=37):\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    \n","def add_dummies(chunk, class_columns):\n","    df = chunk[['quotation', 'qids', 'occupation', 'class']].copy()\n","    df = df.dropna()\n","    df = pd.concat([df.reset_index(drop=True), pd.get_dummies(df['class'].reset_index(drop=True), prefix='class')], axis=1)\n","    for cl in class_columns:\n","        if cl not in df.columns:\n","            df[cl] = df['class'] * 0\n","    return df.copy()"],"metadata":{"id":"11zS09loYEG7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We decided to make the balanced train and test datasets to prevent the model overfits to the most frequent classes and do correct model validation"],"metadata":{"id":"lHgE2uZPxIU4"}},{"cell_type":"code","source":["# split all data into train and validation (test) parts per each year and save it in files\n","# We remove quotations with length less than 'q_length_min' letters (50 letters)\n","\n","start_time = time.time()\n","for year in years:\n","    header_train = True\n","    header_val = True\n","    total_len_train = 0\n","    total_len_val = 0\n","    print('=' * 50, f\"YEAR {year}\", '=' * 50)\n","    path_to_file = \"filtered_occupancy_\"+ year + \".csv.bz2\"     # the files were provided by my colleague\n","    df_reader = pd.read_csv(path_to_file, compression='bz2', chunksize=chunksize)\n","    chunk_number = 0\n","    for chunk in df_reader:\n","        chunk_number+=1\n","        print('=' * 50, f\"CHUNK {chunk_number}\", '=' * 50)\n","        print(\"chunk shape:\", chunk.shape)\n","        # remove quotations with length less than 'q_length_min' letters\n","        chunk = chunk.drop(chunk[chunk['quotation'].apply(lambda x: len(x)) <= q_length_min].index)\n","        print(\"chunk shape after dropping short quotes:\", chunk.shape)\n","        if chunk.shape[0] == 0:\n","            print(\"continue with next chunk\")\n","            continue\n","        # set random state and split the data into train and validation (test) parts with ratio 'test_size_splitting'\n","        set_seed()\n","        train_df, val_df = train_test_split(chunk, test_size=test_size_splitting)\n","\n","        # save train part\n","        train_df.reset_index(drop=True, inplace=True)\n","        train_df.index = train_df.index + total_len_train\n","        train_df.to_csv(\"train_10classes_filtered_occupancy_\"+ year + \".csv.bz2\", header=header_train, mode='a')\n","        if header_train:\n","            header_train = False\n","        print(\"len(train_df): \", len(train_df))\n","        total_len_train+= len(train_df)\n","\n","        # save validation part\n","        val_df.reset_index(drop=True, inplace=True)\n","        val_df.index = val_df.index + total_len_val\n","        val_df.to_csv(\"test_10classes_filtered_occupancy_\"+ year + \".csv.bz2\", header=header_train, mode='a')\n","        if header_val:\n","            header_val = False\n","        print(\"len(val_df): \", len(val_df))\n","        total_len_val+= len(val_df)\n","\n","        print('=' * 50, f\"END CHUNK {chunk_number}\", '=' * 50)\n","    print('=' * 50, f\"END YEAR {year}\", '=' * 50)\n","    delta_time = time.time() - start_time\n","    print('=' * 50, f\"TIME {delta_time} sec\", '=' * 50)"],"metadata":{"id":"7yyXnXvtw5R6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create separate files with quotations per each class (for train and test data)\n","\n","# create separate files for train data\n","start_time = time.time()\n","total_lens = [0]*len(class_columns)\n","headers = [True]*len(class_columns)\n","for year in years:\n","    header = True\n","    print('=' * 50, f\"YEAR {year}\", '=' * 50)\n","    df_reader = pd.read_csv(\"train_10classes_filtered_occupancy_\"+ year + \".csv.bz2\", compression='bz2', chunksize=chunksize)\n","    chunk_number = 0\n","    for chunk in df_reader:\n","        chunk_number+=1\n","        print('=' * 50, f\"CHUNK {chunk_number}\", '=' * 50)\n","        print(\"chunk shape:\", chunk.shape)\n","        chunk['year'] = year\n","        for i, cls in enumerate(class_columns):\n","            chunk_i = chunk[chunk['class'] == i+1].copy()\n","            if len(chunk_i) == 0:\n","                continue\n","            chunk_i.reset_index(drop=True, inplace=True)\n","            chunk_i.index = chunk_i.index + total_lens[i]\n","            total_lens[i]+= len(chunk_i)\n","            chunk_i.to_csv(\"train_\" + cls + \"_filtered_occupancy.csv.bz2\", header=headers[i], mode='a')\n","            if headers[i]:\n","                headers[i] = False\n","        print('=' * 50, f\"END CHUNK {chunk_number}\", '=' * 50)\n","    print('=' * 50, f\"END YEAR {year}\", '=' * 50)\n","    delta_time = time.time() - start_time\n","    print('=' * 50, f\"TIME {delta_time} sec\", '=' * 50)\n","print(\"total_lens:\")\n","print(total_lens)\n","\n","# >>> total_lens:\n","# >>> [1145315, 532178, 3079250, 90736, 75932, 5795132, 1003257, 80187, 72798, 338875]\n","\n","# create separate files for test data\n","start_time = time.time()\n","total_lens = [0]*len(class_columns)\n","headers = [True]*len(class_columns)\n","for year in years:\n","    header = True\n","    print('=' * 50, f\"YEAR {year}\", '=' * 50)\n","    df_reader = pd.read_csv(\"test_10classes_filtered_occupancy_\"+ year + \".csv.bz2\", compression='bz2', chunksize=chunksize)\n","    chunk_number = 0\n","    for chunk in df_reader:\n","        chunk_number+=1\n","        print('=' * 50, f\"CHUNK {chunk_number}\", '=' * 50)\n","        print(\"chunk shape:\", chunk.shape)\n","        chunk['year'] = year\n","        for i, cls in enumerate(class_columns):\n","            chunk_i = chunk[chunk['class'] == i+1].copy()\n","            if len(chunk_i) == 0:\n","                continue\n","            chunk_i.reset_index(drop=True, inplace=True)\n","            chunk_i.index = chunk_i.index + total_lens[i]\n","            total_lens[i]+= len(chunk_i)\n","            chunk_i.to_csv(\"test_\" + cls + \"_filtered_occupancy.csv.bz2\", header=headers[i], mode='a')\n","            if headers[i]:\n","                headers[i] = False\n","        print('=' * 50, f\"END CHUNK {chunk_number}\", '=' * 50)\n","    print('=' * 50, f\"END YEAR {year}\", '=' * 50)\n","    delta_time = time.time() - start_time\n","    print('=' * 50, f\"TIME {delta_time} sec\", '=' * 50)\n","print(\"total_lens:\")\n","print(total_lens)\n","\n","# >>> total_lens:\n","# >>> [1143737, 531858, 3080498, 90205, 76068, 5798577, 1002162, 79443, 72576, 339995]"],"metadata":{"id":"ErhMrxdN0ojI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the smallest class is class_9 with 72798 samples in train part and 72576 samples in test part\n","# final balanced train and test datasets will have 72000 samples per each class\n","class_min_size = 72000"],"metadata":{"id":"ioxJ-sbo2qQd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read the same amount of each class from train data\n","total_len = 0\n","dfs = []\n","for i, cls in enumerate(class_columns):\n","    print(cls)\n","    path_to_file = path_to_output + \"train_\" + cls + \"_filtered_occupancy.csv.bz2\"\n","    df = pd.read_csv(path_to_file, compression='bz2')\n","    # set random state and take 'class_min_size' indexes\n","    set_seed()\n","    indexes = np.arange(len(df))\n","    np.random.shuffle(indexes)\n","    indexes = indexes[:class_min_size]\n","\n","    df = df.iloc[indexes]\n","    print(\"len(df): \", len(df))\n","    df.reset_index(drop=True, inplace=True)\n","    df.index = df.index + total_len\n","    dfs.append(df.copy())\n","\n","# save balanced train dataset\n","header = True\n","for i in range(class_min_size//2):\n","    df = dfs[0].iloc[2*i : 2*(i+1)]\n","    for j in range(1, 10):\n","        df = pd.concat([df, dfs[j].iloc[2*i : 2*(i+1)]], axis=0)\n","    df.to_csv(\"train_balanced_72000x10.csv.bz2\", header=header, mode='a')\n","    if header:\n","        header = False\n","\n","## do the same for test data\n","\n","# read the same amount of each class from test data\n","class_min_size = 72000\n","total_len = 0\n","dfs = []\n","for i, cls in enumerate(class_columns):\n","    print(cls)\n","    path_to_file = path_to_output + \"test_\" + cls + \"_filtered_occupancy.csv.bz2\"\n","    df = pd.read_csv(path_to_file, compression='bz2')\n","    # set random state and take 'class_min_size' indexes\n","    set_seed()\n","    indexes = np.arange(len(df))\n","    np.random.shuffle(indexes)\n","    indexes = indexes[:class_min_size]\n","\n","    df = df.iloc[indexes]\n","    print(\"len(df): \", len(df))\n","    df.reset_index(drop=True, inplace=True)\n","    df.index = df.index + total_len\n","    dfs.append(df.copy())\n","\n","# save balanced test dataset\n","header = True\n","for i in range(class_min_size//2):\n","    df = dfs[0].iloc[2*i : 2*(i+1)]\n","    for j in range(1, 10):\n","        df = pd.concat([df, dfs[j].iloc[2*i : 2*(i+1)]], axis=0)\n","    df.to_csv(\"test_balanced_72000x10.csv.bz2\", header=header, mode='a')\n","    if header:\n","        header = False"],"metadata":{"id":"C0LHsgRA2g4_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define parameters\n","num_of_classes = 10\n","class_columns = [f\"class_{i}\" for i in range(1, num_of_classes+1)]\n","years = [f\"20{i}\" for i in range(15, 21)]\n","\n","chunksize = 10000\n","delta_weights = chunksize // 100    # a parameter needed to calculate class weights in case of unbalanced data \n","\n","BATCH_SIZE = 20\n","q_length = 400\n","q_length_min = 50\n","\n","EPOCH_NUM = 2\n","n_chunk = data_size // chunksize + 1\n","n_batch = chunksize // BATCH_SIZE + 1\n","total_steps = int(n_chunk * n_batch) * EPOCH_NUM\n","\n","warmup_steps = total_steps // 10\n","val_in_steps = 6\n","\n","test_size_splitting = 0.5"],"metadata":{"id":"g6ucond6YEG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# choose a device\n","device = torch.device('cpu')\n","if torch.cuda.is_available():\n","    device = torch.device('cuda:0')\n","\n","# set the model\n","bert_model_name = 'bert-base-cased'\n","tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n","model = BertClassifier(BertModel.from_pretrained(bert_model_name), num_of_classes).to(device)\n","\n","# set an optimizer and a scheduler\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n","scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)"],"metadata":{"id":"peF5ipVrYEG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## load previously trained model params (if needed)\n","# PATH = \"model_state_dict\"\n","# checkpoint = torch.load(PATH, map_location=device)\n","# model.load_state_dict(checkpoint['model_state_dict'])\n","# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","# scheduler.load_state_dict(checkpoint['scheduler_state_dict'])"],"metadata":{"id":"kkYaM9nSYEHB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train the model\n","eval_loss = None\n","start_time = time.time()\n","val_step = 0\n","for epoch in range(EPOCH_NUM):\n","    print('=' * 50, f\"EPOCH {epoch+1} / {EPOCH_NUM}\", '=' * 50)\n","    path_to_file = \"train_balanced_72000x10.csv.bz2\"\n","    df_reader = pd.read_csv(path_to_file, compression='bz2', chunksize=chunksize)\n","    chunk_number = 0\n","    for chunk in df_reader:\n","        val_step+=1\n","        chunk_number+=1\n","        print('=' * 50, f\"CHUNK {chunk_number}\", '=' * 50)\n","        print(\"chunk shape:\", chunk.shape)\n","        df = add_dummies(chunk, class_columns)        \n","        chunk = None # free the memory\n","        \n","        # print classes' percentages into the chunk \n","        print((df[class_columns].sum() / df.shape[0] * 100).apply(lambda x: f\"{x}\") + ' %')\n","        \n","        # calculate class weights for loss function\n","        weights = ((df[class_columns]).sum() + delta_weights).to_numpy() / (len(df) + delta_weights*num_of_classes)\n","        weights = [1./w for w in weights]\n","        # normalize the weights \n","        sum_weight = sum(weights)\n","        weights = [w/sum_weight for w in weights]\n","        print(\"weights:\", weights)\n","        # set loss function: weighted binary cross entropy\n","        criterion = nn.BCEWithLogitsLoss(weight=torch.Tensor(weights).to(device))\n","\n","        train_dataset = QuoteDataset(tokenizer, df, class_columns,  q_length=q_length, lazy=True)\n","        df = None # free the memory\n","        collate_fn = partial(collate_fn, device=device)\n","        train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","        train_dataset = None # free the memory\n","\n","        train(model, train_iterator, optimizer, scheduler, criterion=criterion)\n","      \n","        delta_time = time.time() - start_time\n","        print('=' * 50, f\"END CHUNK {chunk_number}\", '=' * 50)        \n","        print('=' * 50, f\"TIME {delta_time} sec\", '=' * 50)\n","\n","        if chunk_number % val_in_steps == 0:\n","            # validation with a part of test data (only first chunk)\n","            path_to_file_eval = path_to_input + \"test_balanced_72000x10.csv.bz2\"\n","            df_reader_eval = pd.read_csv(path_to_file_eval, compression='bz2', chunksize=chunksize)\n","            for chunk_eval in df_reader_eval:\n","                print(\"chunk_eval shape:\", chunk_eval.shape)\n","                df_eval = add_dummies(chunk_eval, class_columns)\n","                chunk_eval = None # free the memory\n","                dev_dataset = QuoteDataset(tokenizer, df_eval, class_columns, q_length=q_length, lazy=True)\n","                dev_iterator = DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","                eval_loss, true, pred = evaluate(model, dev_iterator, class_columns, criterion=criterion)\n","                break\n","            df_reader_eval = None # free the memory\n","            df_eval = None # free the memory\n","            delta_time = time.time() - start_time\n","            print('=' * 50, f\"TIME {delta_time} sec\", '=' * 50)\n","            # save the model\n","            save_model(model, optimizer, scheduler, eval_loss, epoch+1, chunk_number, delta_time)\n","    print('=' * 50, f\"END EPOCH {epoch+1} / {EPOCH_NUM}\", '=' * 50)"],"metadata":{"id":"WL4RWzP6YEHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# final validation with all test data\n","path_to_file_eval = path_to_input + \"test_balanced_72000x10.csv.bz2\"\n","chunk_eval = pd.read_csv(path_to_file_eval, compression='bz2')\n","df_eval = add_dummies(chunk_eval, class_columns)\n","chunk_eval = None # free the memory\n","dev_dataset = QuoteDataset(tokenizer, df_eval, class_columns, q_length=q_length, lazy=True)\n","dev_iterator = DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","eval_loss, true, pred = evaluate_return(model, dev_iterator, class_columns, criterion=criterion)\n","\n","# save true and predicted classes to plot ROC curves and compute the confusion matrix later\n","np.save(path_to_output + f\"true__test_72000x10_epoch_{epoch+1}\", true)\n","np.save(path_to_output + f\"pred__test_72000x10_epoch_{epoch+1}\", pred)"],"metadata":{"id":"-VYhqPL8YEHC"},"execution_count":null,"outputs":[]}]}